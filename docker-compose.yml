
services:
  litellm:
    build:
      context: .
      args:
        target: runtime
    image: ghcr.io/berriai/litellm:main-stable
    #########################################
    ## Uncomment these lines to start proxy with a config.yaml file ##
    # volumes:
    #   - ./proxy_server_config.yaml:/app/config.yaml
    # command: [ "--config", "./config.yaml", "--port", "4000"]
    ###############################################
    ports:
      - "4000:4000" # Map the container port to the host, change the host port if necessary
    volumes:
      - ./chatthing-config.yaml:/app/config.yaml # Mount the local configuration file
    # You can change the port or number of workers as per your requirements or pass any new supported CLI augument. Make sure the port passed here matches with the container port defined above in `ports` value
    command: [ "--config", "/app/config.yaml", "--port", "4000", "--num_workers", "8" ]
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - COHERE_API_KEY=${COHERE_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
