model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
  - model_name: haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      max_tokens: 4096
      # add_function_to_prompt: True
      # api_base: https://api.anthropic.com/v1/messages
      # api_version: "2023-05-15"
      api_key: os.environ/ANTHROPIC_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
  - model_name: opus
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      # api_base: https://api.anthropic.com/v1/messages
      # api_version: "2023-05-15"
      api_key: os.environ/ANTHROPIC_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
  - model_name: llama2
    litellm_params:
      model: groq/llama2-70b-4096
      # api_base: https://api.anthropic.com/v1/messages
      # api_version: "2023-05-15"
      api_key: os.environ/GROQ_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
  - model_name: mixtral
    litellm_params:
      model: groq/mixtral-8x7b-32768
      # api_base: https://api.anthropic.com/v1/messages
      # api_version: "2023-05-15"
      api_key: os.environ/GROQ_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
  - model_name: command-r
    litellm_params:
      model: cohere/command-r-plus
      # api_base: https://api.anthropic.com/v1/messages
      # api_version: "2023-05-15"
      api_key: os.environ/COHERE_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
  - model_name: gpt-3.5-turbo-large
    litellm_params: 
      model: "gpt-3.5-turbo-1106"
      api_key: os.environ/OPENAI_API_KEY
      rpm: 480
      timeout: 300
      stream_timeout: 60 # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
  - model_name: text-embedding-ada-002
    litellm_params: 
      model: openai/text-embedding-ada-002
      api_key: os.environ/OPENAI_API_KEY
      # api_base: https://openai-gpt-4-test-v-1.openai.azure.com/
      # api_version: "2023-05-15"
    model_info:
      mode: embedding
      base_model: text-embedding-ada-002

litellm_settings:
  # drop_params: True
  # max_budget: 100 
  # budget_duration: 30d
  modify_params: True
  set_verbose: True
  num_retries: 5
  request_timeout: 600
  telemetry: False
  context_window_fallbacks: [{"gpt-3.5-turbo": ["gpt-3.5-turbo-large"]}]

general_settings: 
  # master_key: sk-1234 # [OPTIONAL] Use to enforce auth on proxy. See - https://docs.litellm.ai/docs/proxy/virtual_keys
  store_model_in_db: True
  proxy_budget_rescheduler_min_time: 60
  proxy_budget_rescheduler_max_time: 64
  proxy_batch_write_at: 1
  # database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # [OPTIONAL] use for token-based auth to proxy

# environment_variables:
  # settings for using redis caching
  # REDIS_HOST: redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com
  # REDIS_PORT: "16337"
  # REDIS_PASSWORD: 
